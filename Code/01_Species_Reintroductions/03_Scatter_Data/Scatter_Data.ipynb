{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\r\n",
    "while os.getcwd().split('\\\\')[-1] != 'ecological-networks':\r\n",
    "    %cd ..\r\n",
    "import setup_paths\r\n",
    "setup_paths.add_path()\r\n",
    "from Species_Reintroductions import *\r\n",
    "DATA_TYPE = 'Real_1D'\r\n",
    "CASE_TO_SOLVE_init(DATA_TYPE)\r\n",
    "get_CASE()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def G_iter(data_two):\r\n",
    "    for key in list(data_two.keys()):\r\n",
    "        for ensembles_ID in range(len(data_two[key])):\r\n",
    "            yield (key, ensembles_ID, 0)\r\n",
    "\r\n",
    "def get_ScatterData(NetworkName, loc):\r\n",
    "    Data_Scatter = []\r\n",
    "    #print(NetworkName)\r\n",
    "    M_df = M[NetworkName]\r\n",
    "    data_two = UnpickleObj(os.path.join(os.getcwd(), 'Code', '01_Species_Reintroductions', '02_Generate_Database', f\"{DATA_TYPE}_Data\", f\"{NetworkName}\", f\"{NetworkName}-Data_two.pkl\"))\r\n",
    "    #data_two = UnpickleObj(f\"{NetworkName}-Data_two.pkl\")\r\n",
    "    Keys_Filter = [[0,1,2], [2,5,8], [0,2]]\r\n",
    "    Ensembles_batch = [0,10]\r\n",
    "    data_two = {k: v[Ensembles_batch[0]:min(Ensembles_batch[1],len(v))] for k,v in data_two.items() if k in set(list(itertools.product(*Keys_Filter)))}\r\n",
    "    del Keys_Filter, Ensembles_batch\r\n",
    "    \r\n",
    "    for approach in ['three','four','five','six','rand']:\r\n",
    "        for key, E, T in itertools.islice(G_iter(data_two),0,None,1):\r\n",
    "            try:\r\n",
    "                X = UnpickleObj(os.path.join(os.getcwd(), 'Code', '01_Species_Reintroductions', '02_Generate_Database', f\"{DATA_TYPE}_Data\", NetworkName, f\"{NetworkName}-Data_{approach}_{key}_{E}_{T}.pkl\"))\r\n",
    "                M_per = RemovalFromNodeList(data_two[key][E],M_df,axis=key[2])\r\n",
    "                M_per = M_per.loc[(M_per!=0).any(axis=1)] #Cleaning Row isoloates\r\n",
    "                M_per = M_per.loc[:, (M_per!=0).any(axis=0)] #Cleaning Column isoloates\r\n",
    "                \r\n",
    "                M_per_dict = {}\r\n",
    "                M_per_dict['n'] = M_per.shape[0]\r\n",
    "                M_per_dict['m'] = M_per.shape[1]\r\n",
    "                M_per_dict['L'] = sum(np.array(M_per).flatten())\r\n",
    "                M_per_dict['S'] = M_per_dict['n'] + M_per_dict['m']\r\n",
    "                M_per_dict['A'] = M_per_dict['m'] / M_per_dict['n']\r\n",
    "                M_per_dict['C'] = M_per_dict['L'] / (M_per_dict['m']*M_per_dict['n'])\r\n",
    "                M_per_dict['N'] = NestednessCalculator(np.array(M_per)).nodf(np.array(M_per))\r\n",
    "                \r\n",
    "                Z1 = np.mean(np.array(X[3][0][-1,:]))\r\n",
    "                Z2 = np.mean(np.array(list(map(isExtinct,X[3][0][-1,:]))))\r\n",
    "                Z3 = getResilience(X[3][0])\r\n",
    "                Z4 = getPersistence(np.array(list(map(isExtinct,X[3][0][-1,:])))) / M_df.shape[T]\r\n",
    "                \r\n",
    "                temp = [M_per_dict[x] for x in ['n','m','L','S','A','C','N']] + [Z1,Z2,Z3,Z4]\r\n",
    "                Data_Scatter += [temp]\r\n",
    "                \r\n",
    "                for Step in list(range(1,len(X[0])+1)):\r\n",
    "                    M_per = M_per.append(M_df.loc[X[0][Step-1]])\r\n",
    "                    M_per[M_per.isnull()] = M_df\r\n",
    "                    M_per = M_per.loc[(M_per!=0).any(axis=1)] #Cleaning Row isoloates\r\n",
    "                    M_per = M_per.loc[:, (M_per!=0).any(axis=0)] #Cleaning Column isoloates\r\n",
    "                    \r\n",
    "                    M_per_dict['n'] = M_per.shape[0]\r\n",
    "                    M_per_dict['m'] = M_per.shape[1]\r\n",
    "                    M_per_dict['L'] = sum(np.array(M_per).flatten())\r\n",
    "                    M_per_dict['S'] = M_per_dict['n'] + M_per_dict['m']\r\n",
    "                    M_per_dict['A'] = M_per_dict['m'] / M_per_dict['n']\r\n",
    "                    M_per_dict['C'] = M_per_dict['L'] / (M_per_dict['m']*M_per_dict['n'])\r\n",
    "                    M_per_dict['N'] = NestednessCalculator(np.array(M_per)).nodf(np.array(M_per))\r\n",
    "                    \r\n",
    "                    Z1 = np.mean(np.array(X[3][Step][-1,:]))\r\n",
    "                    Z2 = np.mean(np.array(list(map(isExtinct,X[3][Step][-1,:]))))\r\n",
    "                    Z3 = getResilience(X[3][Step])\r\n",
    "                    Z4 = getPersistence(np.array(list(map(isExtinct,X[3][Step][-1,:])))) / M_df.shape[T]\r\n",
    "                    \r\n",
    "                    temp = [M_per_dict[x] for x in ['n','m','L','S','A','C','N']] + [Z1,Z2,Z3,Z4]\r\n",
    "                    Data_Scatter += [temp]\r\n",
    "            except:\r\n",
    "                #print(key, E, T)\r\n",
    "                pass\r\n",
    "    DF = pd.DataFrame(Data_Scatter, columns = ['n','m','L','S','A','C','N','Abundance','Ob Abundance','Settling Time','Persistence'])\r\n",
    "    DF.drop_duplicates(inplace=True, ignore_index=True)\r\n",
    "    #print(NetworkName, len(Data_Scatter), DF.shape[0])\r\n",
    "    FileName = os.path.join(os.getcwd(), 'Code', '01_Species_Reintroductions', '03_Scatter_Data', 'ScatterData', f\"{loc}\", f\"{NetworkName}.pkl\")\r\n",
    "    os.makedirs(os.path.dirname(FileName), exist_ok=True)\r\n",
    "    PickleObj(DF,FileName)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for CASE_TO_SOLVE, loc in zip(['Real_1D', 'Syn_01', 'Syn_2D', 'Real_2D'],['01','02','03','04']):\r\n",
    "    DATA_TYPE = CASE_TO_SOLVE\r\n",
    "    CASE_TO_SOLVE_init(DATA_TYPE)\r\n",
    "    NetworkName_Solved_df = UnpickleObj(os.path.join(os.getcwd(), 'Code', '01_Species_Reintroductions', '01_Simulation_Execution', f\"Network_Solved_{DATA_TYPE.split('_')[0]}.pkl\"))\r\n",
    "    M = UnpickleObj(os.path.join(os.getcwd(), 'Code', '01_Species_Reintroductions', '01_Simulation_Execution', f\"M_{DATA_TYPE.split('_')[0]}.pkl\"))\r\n",
    "\r\n",
    "    if DATA_TYPE.split('_')[0] == 'Syn':\r\n",
    "        M = {k:M[k] for k in M.keys() if int(k.split('_')[2]) == 100}\r\n",
    "\r\n",
    "    with Parallel(n_jobs=-1, verbose = 10) as parallel:\r\n",
    "            data = parallel(delayed(get_ScatterData)(NetworkName,loc) for NetworkName in itertools.islice(M.keys(),0,None,1))\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "UPDATE_SAVED = False\r\n",
    "for CASE_TO_SOLVE, loc in zip(['Real_1D', 'Syn_1D', 'Syn_2D', 'Real_2D'],['01','02','03','04']):\r\n",
    "    DATA_TYPE = CASE_TO_SOLVE\r\n",
    "    CASE_TO_SOLVE_init(DATA_TYPE)\r\n",
    "    NetworkName_Solved_df = UnpickleObj(os.path.join(os.getcwd(), 'Code', '01_Species_Reintroductions', '01_Simulation_Execution', f\"Network_Solved_{DATA_TYPE.split('_')[0]}.pkl\"))\r\n",
    "    M = UnpickleObj(os.path.join(os.getcwd(), 'Code', '01_Species_Reintroductions', '01_Simulation_Execution', f\"M_{DATA_TYPE.split('_')[0]}.pkl\"))\r\n",
    "\r\n",
    "    D = {}\r\n",
    "    DF = pd.DataFrame([],columns = ['n','m','L','S','A','C','N','Abundance','Ob Abundance','Settling Time','Persistence'])\r\n",
    "\r\n",
    "    for NetworkName in itertools.islice(M.keys(),0,None,1):\r\n",
    "        X = UnpickleObj(f\"ScatterData/{loc}/{NetworkName}.pkl\")\r\n",
    "        X = X.apply(pd.to_numeric)\r\n",
    "        X.drop(X[ X['Ob Abundance'] == 0 ].index ,inplace = True)\r\n",
    "        Y1 = X.corr(method = 'pearson')\r\n",
    "        Y2 = X.corr(method = 'kendall')\r\n",
    "        Y3 = X.corr(method = 'spearman')\r\n",
    "        D[NetworkName] = [Y1,Y2,Y3]\r\n",
    "        DF = pd.concat([DF,X], ignore_index = True)\r\n",
    "    del NetworkName, X, Y1, Y2, Y3, M, NetworkName_Solved_df\r\n",
    "    DF = DF.apply(pd.to_numeric)\r\n",
    "    DF.sort_values(by = ['Abundance', 'Ob Abundance'], axis = 0, inplace = True, ignore_index = True)\r\n",
    "    DF.drop(DF[ DF['Ob Abundance'] == 0 ].index ,inplace = True)\r\n",
    "    DF.reset_index(drop = True, inplace = True)\r\n",
    "\r\n",
    "    Z = DF.describe()\r\n",
    "    Y1 = DF.corr(method = 'pearson')\r\n",
    "    Y2 = DF.corr(method = 'kendall')\r\n",
    "    Y3 = DF.corr(method = 'spearman')\r\n",
    "    Data = [Z, Y1, Y2, Y3, DF, D]\r\n",
    "    if UPDATE_SAVED:\r\n",
    "        PickleObj(Data, os.path.join(os.getcwd(), 'Code', '01_Species_Reintroductions', '03_Scatter_Data', f\"Scatter_{loc}_Consolidated.pkl\"))\r\n",
    "del loc, CASE_TO_SOLVE, DF, Z, Y1, Y2, Y3, Data\r\n",
    "del D, UPDATE_SAVED"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "FileName = os.path.join(os.getcwd(), 'Code', '01_Species_Reintroductions', '03_Scatter_Data', f\"Scatter_01_Consolidated.pkl\")\r\n",
    "D01 = UnpickleObj(FileName)[4]\r\n",
    "D01.drop(D01[ D01['N'] == 0 ].index ,inplace = True)\r\n",
    "D01.dropna(axis = 0, inplace = True)\r\n",
    "D01 = D01[D01['Abundance'] > 1.0]\r\n",
    "D01.reset_index(drop = True, inplace = True)\r\n",
    "\r\n",
    "FileName = os.path.join(os.getcwd(), 'Code', '01_Species_Reintroductions', '03_Scatter_Data', f\"Scatter_02_Consolidated.pkl\")\r\n",
    "D02 = UnpickleObj(FileName)[4]\r\n",
    "D02.drop(D02[ D02['N'] == 0 ].index ,inplace = True)\r\n",
    "D02.dropna(axis = 0, inplace = True)\r\n",
    "D02 = D02[D02['Abundance'] > 1.0]\r\n",
    "D02.reset_index(drop = True, inplace = True)\r\n",
    "\r\n",
    "FileName = os.path.join(os.getcwd(), 'Code', '01_Species_Reintroductions', '03_Scatter_Data', f\"Scatter_03_Consolidated.pkl\")\r\n",
    "D03 = UnpickleObj(FileName)[4]\r\n",
    "D03.drop(D03[ D03['N'] == 0 ].index ,inplace = True)\r\n",
    "D03.dropna(axis = 0, inplace = True)\r\n",
    "D03 = D03[D03['Abundance'] > 0.3]\r\n",
    "D03.reset_index(drop = True, inplace = True)\r\n",
    "\r\n",
    "FileName = os.path.join(os.getcwd(), 'Code', '01_Species_Reintroductions', '03_Scatter_Data', f\"Scatter_04_Consolidated.pkl\")\r\n",
    "D04 = UnpickleObj(FileName)[4]\r\n",
    "D04.drop(D04[ D04['N'] == 0 ].index ,inplace = True)\r\n",
    "D04.dropna(axis = 0, inplace = True)\r\n",
    "D04 = D04[D04['Abundance'] > 0.3]\r\n",
    "D04.reset_index(drop = True, inplace = True)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.6.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.6.13 64-bit ('master': conda)"
  },
  "interpreter": {
   "hash": "0c640dc386691b1b42b3424c708d4d1eda291d0197a35eddf54ba99eb6d636bd"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}