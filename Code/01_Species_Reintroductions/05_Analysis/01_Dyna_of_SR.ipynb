{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamics of Species Reintroduciton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/udit/Desktop/Sarth/ecological-networks/Code/01_Species_Reintroductions\n",
      "/home/udit/Desktop/Sarth/ecological-networks/Code\n",
      "/home/udit/Desktop/Sarth/ecological-networks\n",
      "Real_1D\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "while not (os.getcwd().split('\\\\')[-1] == 'ecological-networks' or os.getcwd().split('/')[-1] == 'ecological-networks'):\n",
    "    %cd ..\n",
    "import setup_paths\n",
    "setup_paths.add_path()\n",
    "from Species_Reintroductions import *\n",
    "global DATA_TYPE\n",
    "DATA_TYPE = 'Real_1D'\n",
    "CASE_TO_SOLVE_init(DATA_TYPE)\n",
    "get_CASE()\n",
    "\n",
    "def CASE_init(CASE_TO_SOLVE):\n",
    "    DATA_TYPE = CASE_TO_SOLVE\n",
    "    CASE_TO_SOLVE_init(DATA_TYPE)\n",
    "    NetworkName_Solved_df = UnpickleObj(os.path.join(os.getcwd(), 'Code', '01_Species_Reintroductions', '01_Simulation_Execution', f\"Network_Solved_{DATA_TYPE.split('_')[0]}.pkl\"))\n",
    "    M = UnpickleObj(os.path.join(os.getcwd(), 'Code', '01_Species_Reintroductions', '01_Simulation_Execution', f\"M_{DATA_TYPE.split('_')[0]}.pkl\"))\n",
    "    if DATA_TYPE.split('_')[0] == 'Syn':\n",
    "        M = {k:M[k] for k in M.keys() if int(k.split('_')[2]) == 100}\n",
    "    return DATA_TYPE, NetworkName_Solved_df, M\n",
    "\n",
    "from scipy.optimize import fsolve\n",
    "from scipy import stats\n",
    "import scipy.linalg as la\n",
    "from sklearn.linear_model import LinearRegression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ODE_Solve_func(A,t0=0,tf=100,num=30,x_low=10e-6, P = {i:j for i,j in zip(['a', 'b', 'mu', 'h', 'g0', 't', 'k'],[0.3, 1.0, 1e-4, 0.7, 1.0, 0.5, 0.0])}, C_type = 'UA'):\n",
    "    t = np.linspace(t0,tf,num)\n",
    "    n,m = A.shape\n",
    "    x0 = [x_low, x_low]\n",
    "    g_p, g_a = [0.0,0.0]\n",
    "    Degree_P = list(np.sum(np.array(A), axis = 1))\n",
    "    Degree_A = list(np.sum(np.array(A), axis = 0))\n",
    "    if C_type == 'UA':\n",
    "        # Unweighted Averaging\n",
    "        g_p = np.sum(P['g0'] * np.power(np.array(Degree_P),1-P['t'])) / len(Degree_P)\n",
    "        g_a = np.sum(P['g0'] * np.power(np.array(Degree_A),1-P['t'])) / len(Degree_A)\n",
    "    elif C_type == 'DA':\n",
    "        # Degree Weihted Averaging\n",
    "        g_p = np.sum(P['g0'] * np.power(np.array(Degree_P),2-P['t'])) / sum(Degree_P)\n",
    "        g_a = np.sum(P['g0'] * np.power(np.array(Degree_A),2-P['t'])) / sum(Degree_A)\n",
    "    elif C_type == 'EA':\n",
    "    # Eigenvector based Averaging\n",
    "        M_P = np.matmul(np.array(M_df),np.array(M_df.T))\n",
    "        M_A = np.matmul(np.array(M_df.T),np.array(M_df))\n",
    "        _, V_P = la.eig(M_P)\n",
    "        _, V_A = la.eig(M_A)\n",
    "        g_p = np.sum( P['g0'] * np.multiply( np.power(np.array(Degree_P),1-P['t']), list(V_P[:,0].real) ) ) / sum(list(V_P[:,0].real))\n",
    "        g_a = np.sum( P['g0'] * np.multiply( np.power(np.array(Degree_A),1-P['t']), list(V_A[:,0].real) ) ) / sum(list(V_A[:,0].real))\n",
    "\n",
    "    def eqs(x, t, x0, P, g_p, g_a):\n",
    "        Pe, Ae = x\n",
    "        return [ P['a']*Pe - P['b']*(Pe**2) + P['mu'] + (g_p*Ae*Pe)/(1 + P['h']*g_p*Ae), \n",
    "                (P['a']-P['k'])*Ae - P['b']*(Ae**2) + P['mu'] + (g_a*Pe*Ae)/(1 + P['h']*g_a*Pe) ]\n",
    "    xl = odeint(eqs,x0,t,args=(A, P, g_p, g_a, ))\n",
    "    return xl\n",
    "P = {i:j for i,j in zip(['a', 'b', 'mu', 'h', 'g0', 't', 'k'],[0.3, 1.0, 1e-4, 0.7, 1.0, 0.5, 0.0])}\n",
    "DATA_TYPE, _, M = CASE_init('Real_2D')\n",
    "NetworkName = 'M_PL_041'\n",
    "M_df = M[NetworkName]\n",
    "X = ODE_Solve_func(\n",
    "    A = M_df,\n",
    "    t0=0,\n",
    "    tf=100,\n",
    "    num=100,\n",
    "    x_low=10e-6, \n",
    "    P = P,\n",
    "    C_type = 'UA'\n",
    "    )\n",
    "print(X[-1,:])\n",
    "plt.plot(X[:,0],'--b')\n",
    "plt.plot(X[:,1],'--r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def f(g_p, g_a):\n",
    "    t = np.linspace(0,100,100)\n",
    "    x0 = [10e-6, 10e-6]\n",
    "    P = {i:j for i,j in zip(['a', 'b', 'mu', 'h', 'g0', 't', 'k'],[0.3, 1.0, 1e-4, 0.7, 1.0, 0.5, 0.0])}\n",
    "    def eqs(x, t):\n",
    "        Pe, Ae = x\n",
    "        return [ P['a']*Pe - P['b']*(Pe**2) + P['mu'] + (g_p*Ae*Pe)/(1 + P['h']*g_p*Ae), \n",
    "                (P['a']-P['k'])*Ae - P['b']*(Ae**2) + P['mu'] + (g_a*Pe*Ae)/(1 + P['h']*g_a*Pe) ]\n",
    "    XL = odeint(eqs,x0,t)\n",
    "    Peff, Aeff = XL[-1,:]\n",
    "    return Peff+Aeff\n",
    "\n",
    "x = np.arange(0.0, 10.0, 0.5) # g_p\n",
    "y = np.arange(0.0, 10.0, 0.5) # g_a\n",
    "\n",
    "X, Y = np.meshgrid(x, y)\n",
    "xf = X.flatten()\n",
    "yf = Y.flatten()\n",
    "Z = [f(xx,yy) for xx,yy in zip(xf,yf)]\n",
    "\n",
    "Z = np.reshape(Z, X.shape)\n",
    "#'''\n",
    "fig = plt.figure(figsize = (10,10))\n",
    "'''\n",
    "ax = plt.axes(projection='3d')\n",
    "#ax.contour3D(X, Y, Z, 50)\n",
    "ax.plot_surface(X,Y,Z)\n",
    "ax.set_xlabel('g_p')\n",
    "ax.set_ylabel('g_a')\n",
    "#ax.set_zlabel('<X>')\n",
    "'''\n",
    "#'''\n",
    "ax = fig.add_axes([1,1,1,1])\n",
    "cp = ax.contourf(X,Y,Z)\n",
    "fig.colorbar(cp)\n",
    "#ax.plot_surface(X,Y,Z)\n",
    "ax.set_xlabel(r'$<\\gamma_P>$', fontsize = 20)\n",
    "ax.set_ylabel(r'$<\\gamma_A>$', fontsize = 20)\n",
    "#'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "DATA_TYPE, _, M = CASE_init('Real_1D')\n",
    "def f(beta_eff, x0 = 1.0):\n",
    "    t = np.linspace(0,100,100)\n",
    "    P = {i:j for i,j in zip(['B', 'C', 'D', 'E', 'H', 'K'],[0.1, 1.0, 5.0, 0.9, 0.1, 5])}\n",
    "    def eqs(x, t):\n",
    "        return P['B'] + x*(1 - (x/P['K']))*((x/P['C'])-1) + beta_eff*((x**2)/(P['D'] + (P['E']+P['H'])*x)) \n",
    "    X_eff = odeint(eqs,x0,t)\n",
    "    return X_eff\n",
    "'''\n",
    "for beta_eff in np.arange(0.2, 2.0, 0.1):\n",
    "    plt.figure(figsize = (10,10))\n",
    "    for x0 in np.arange(0.0, 4.0, 0.1):\n",
    "        X = f(beta_eff, x0)\n",
    "        plt.plot(X)\n",
    "'''\n",
    "x = np.arange(0.0, 10.0, 0.1) # beta_eff\n",
    "y = np.array([f(xx)[-1,0] for xx in x]) # <X>\n",
    "plt.figure()\n",
    "plt.plot(x,y)\n",
    "plt.xlabel(r'$\\beta_{eff}$', fontsize = 20)\n",
    "plt.ylabel(r'$<X>$', fontsize = 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def G_iter(data_two):\n",
    "    for key in list(data_two.keys()):\n",
    "        for ensembles_ID in range(len(data_two[key])):\n",
    "            yield (key, ensembles_ID, 0)\n",
    "\n",
    "def get_Data(NetworkName, loc):\n",
    "    Data_Scatter = []\n",
    "    #print(NetworkName)\n",
    "    M_df = M[NetworkName]\n",
    "    #data_two = UnpickleObj(os.path.join(os.getcwd(), 'Code', '01_Species_Reintroductions', '02_Generate_Database', f\"{DATA_TYPE}_Data\", f\"{NetworkName}\", f\"{NetworkName}-Data_two.pkl\"))\n",
    "    #data_two = UnpickleObj(os.path.join(r\"D:\\WORK\\EN\\Documents\\Ecological_Restoration_01\\Real_Data\", f\"{NetworkName}\\{NetworkName}-Data_two.pkl\"))\n",
    "    data_two = UnpickleObj(f\"/home/udit/Documents/Ecological_Restoration_{loc}/{DATA_TYPE.split('_')[0]}_Data/{NetworkName}/{NetworkName}-Data_two.pkl\")\n",
    "    Keys_Filter = [[0,1,2], [2,5,8], [0,2]]\n",
    "    Ensembles_batch = [0,10]\n",
    "    data_two = {k: v[Ensembles_batch[0]:min(Ensembles_batch[1],len(v))] for k,v in data_two.items() if k in set(list(itertools.product(*Keys_Filter)))}\n",
    "    del Keys_Filter, Ensembles_batch\n",
    "    P = {i:j for i,j in zip(['a', 'b', 'mu', 'h', 'g0', 't', 'k'],[0.3, 1.0, 1e-4, 0.7, 1.0, 0.5, 0.0])}\n",
    "    for approach in ['three','four','five','six','rand']:\n",
    "        for key, E, T in itertools.islice(G_iter(data_two),0,None,1):\n",
    "            try:\n",
    "                #X = UnpickleObj(os.path.join(os.getcwd(), 'Code', '01_Species_Reintroductions', '02_Generate_Database', f\"{DATA_TYPE}_Data\", NetworkName, f\"{NetworkName}-Data_{approach}_{key}_{E}_{T}.pkl\"))\n",
    "                #X = UnpickleObj(os.path.join(r\"D:\\WORK\\EN\\Documents\\Ecological_Restoration_04\\Real_Data\", f\"{NetworkName}\\{NetworkName}-Data_{approach}_{key}_{E}_{T}.pkl\"))\n",
    "                X = UnpickleObj(f\"/home/udit/Documents/Ecological_Restoration_{loc}/{DATA_TYPE.split('_')[0]}_Data/{NetworkName}/{NetworkName}-Data_{approach}_{key}_{E}_{T}.pkl\")\n",
    "                M_per = RemovalFromNodeList(data_two[key][E],M_df,axis=key[2])\n",
    "                M_per = M_per.loc[(M_per!=0).any(axis=1)] #Cleaning Row isoloates\n",
    "                M_per = M_per.loc[:, (M_per!=0).any(axis=0)] #Cleaning Column isoloates\n",
    "                \n",
    "                M_per_dict = {}\n",
    "                \n",
    "                Degree_P = list(np.sum(np.array(M_per), axis = 1))\n",
    "                Degree_A = list(np.sum(np.array(M_per), axis = 0))\n",
    "                g_p = np.sum(P['g0'] * np.power(np.array(Degree_P),1-P['t'])) / len(Degree_P)\n",
    "                g_a = np.sum(P['g0'] * np.power(np.array(Degree_A),1-P['t'])) / len(Degree_A)\n",
    "                M_per_dict['g_p'] = g_p\n",
    "                M_per_dict['g_a'] = g_a\n",
    "                beta_eff_p, _, _ = ODE_Solve_Low(t0=0,tf=100,num=50,x_low=0,A=np.array(getProjectedNet(M_per, flag = [True, False])[0]))\n",
    "                beta_eff_a, _, _ = ODE_Solve_Low(t0=0,tf=100,num=50,x_low=0,A=np.array(getProjectedNet(M_per, flag = [False, True])[1]))\n",
    "                M_per_dict['beta_eff_p'] = beta_eff_p\n",
    "                M_per_dict['beta_eff_a'] = beta_eff_a\n",
    "                \n",
    "                M_per_dict['n'] = M_per.shape[0]\n",
    "                M_per_dict['m'] = M_per.shape[1]\n",
    "                M_per_dict['L'] = sum(np.array(M_per).flatten())\n",
    "                M_per_dict['S'] = M_per_dict['n'] + M_per_dict['m']\n",
    "                M_per_dict['A'] = M_per_dict['m'] / M_per_dict['n']\n",
    "                M_per_dict['C'] = M_per_dict['L'] / (M_per_dict['m']*M_per_dict['n'])\n",
    "                M_per_dict['N'] = NestednessCalculator(np.array(M_per)).nodf(np.array(M_per))\n",
    "                \n",
    "                Z1 = np.mean(np.array(X[3][0][-1,:]))\n",
    "                Z2 = np.mean(np.array(list(map(isExtinct,X[3][0][-1,:]))))\n",
    "                Z3 = getResilience(X[3][0])\n",
    "                Z4 = getPersistence(np.array(list(map(isExtinct,X[3][0][-1,:])))) / M_df.shape[T]\n",
    "                \n",
    "                temp = [M_per_dict[x] for x in ['n','m','L','S','A','C','N','g_p','g_a','beta_eff_p','beta_eff_a']] + [Z1,Z2,Z3,Z4]\n",
    "                Data_Scatter += [temp]\n",
    "                \n",
    "                for Step in list(range(1,len(X[0])+1)):\n",
    "                    M_per = M_per.append(M_df.loc[X[0][Step-1]])\n",
    "                    M_per[M_per.isnull()] = M_df\n",
    "                    M_per = M_per.loc[(M_per!=0).any(axis=1)] #Cleaning Row isoloates\n",
    "                    M_per = M_per.loc[:, (M_per!=0).any(axis=0)] #Cleaning Column isoloates\n",
    "                    \n",
    "                    Degree_P = list(np.sum(np.array(M_per), axis = 1))\n",
    "                    Degree_A = list(np.sum(np.array(M_per), axis = 0))\n",
    "                    g_p = np.sum(P['g0'] * np.power(np.array(Degree_P),1-P['t'])) / len(Degree_P)\n",
    "                    g_a = np.sum(P['g0'] * np.power(np.array(Degree_A),1-P['t'])) / len(Degree_A)\n",
    "                    M_per_dict['g_p'] = g_p\n",
    "                    M_per_dict['g_a'] = g_a\n",
    "                    beta_eff_p, _, _ = ODE_Solve_Low(t0=0,tf=100,num=50,x_low=0,A=np.array(getProjectedNet(M_per, flag = [True, False])[0]))\n",
    "                    beta_eff_a, _, _ = ODE_Solve_Low(t0=0,tf=100,num=50,x_low=0,A=np.array(getProjectedNet(M_per, flag = [False, True])[1]))\n",
    "                    M_per_dict['beta_eff_p'] = beta_eff_p\n",
    "                    M_per_dict['beta_eff_a'] = beta_eff_a\n",
    "\n",
    "                    M_per_dict['n'] = M_per.shape[0]\n",
    "                    M_per_dict['m'] = M_per.shape[1]\n",
    "                    M_per_dict['L'] = sum(np.array(M_per).flatten())\n",
    "                    M_per_dict['S'] = M_per_dict['n'] + M_per_dict['m']\n",
    "                    M_per_dict['A'] = M_per_dict['m'] / M_per_dict['n']\n",
    "                    M_per_dict['C'] = M_per_dict['L'] / (M_per_dict['m']*M_per_dict['n'])\n",
    "                    M_per_dict['N'] = NestednessCalculator(np.array(M_per)).nodf(np.array(M_per))\n",
    "                    \n",
    "                    Z1 = np.mean(np.array(X[3][Step][-1,:]))\n",
    "                    Z2 = np.mean(np.array(list(map(isExtinct,X[3][Step][-1,:]))))\n",
    "                    Z3 = getResilience(X[3][Step])\n",
    "                    Z4 = getPersistence(np.array(list(map(isExtinct,X[3][Step][-1,:])))) / M_df.shape[T]\n",
    "                    \n",
    "                    temp = [M_per_dict[x] for x in ['n','m','L','S','A','C','N','g_p','g_a','beta_eff_p','beta_eff_a']] + [Z1,Z2,Z3,Z4]\n",
    "                    Data_Scatter += [temp]\n",
    "            except:\n",
    "                #print(key, E, T)\n",
    "                pass\n",
    "    DF = pd.DataFrame(Data_Scatter, columns = ['n','m','L','S','A','C','N','g_p','g_a','beta_eff_p','beta_eff_a','Abundance','Ob Abundance','Settling Time','Persistence'])\n",
    "    #DF.drop_duplicates(inplace=True, ignore_index=True)\n",
    "    FileName = os.path.join(os.getcwd(), 'Code', '01_Species_Reintroductions', '05_Analysis', 'Dyna_of_SR', f\"{loc}\", f\"{NetworkName}.pkl\")\n",
    "    os.makedirs(os.path.dirname(FileName), exist_ok=True)\n",
    "    if DF.shape[0] != 0:\n",
    "        PickleObj(DF,FileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ipykernel_launcher:32: RuntimeWarning: invalid value encountered in double_scalars\n",
      "ipykernel_launcher:33: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/home/udit/anaconda3/envs/master/lib/python3.7/site-packages/scipy/integrate/odepack.py:247: ODEintWarning: Illegal input detected (internal error). Run with full_output = 1 to get quantitative information.\n",
      "  warnings.warn(warning_msg, ODEintWarning)\n",
      "ipykernel_launcher:32: RuntimeWarning: invalid value encountered in double_scalars\n",
      "ipykernel_launcher:33: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/home/udit/anaconda3/envs/master/lib/python3.7/site-packages/scipy/integrate/odepack.py:247: ODEintWarning: Illegal input detected (internal error). Run with full_output = 1 to get quantitative information.\n",
      "  warnings.warn(warning_msg, ODEintWarning)\n",
      "ipykernel_launcher:32: RuntimeWarning: invalid value encountered in double_scalars\n",
      "ipykernel_launcher:33: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/home/udit/anaconda3/envs/master/lib/python3.7/site-packages/scipy/integrate/odepack.py:247: ODEintWarning: Illegal input detected (internal error). Run with full_output = 1 to get quantitative information.\n",
      "  warnings.warn(warning_msg, ODEintWarning)\n",
      "ipykernel_launcher:32: RuntimeWarning: invalid value encountered in double_scalars\n",
      "ipykernel_launcher:33: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/home/udit/anaconda3/envs/master/lib/python3.7/site-packages/scipy/integrate/odepack.py:247: ODEintWarning: Illegal input detected (internal error). Run with full_output = 1 to get quantitative information.\n",
      "  warnings.warn(warning_msg, ODEintWarning)\n",
      "ipykernel_launcher:32: RuntimeWarning: invalid value encountered in double_scalars\n",
      "ipykernel_launcher:33: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/home/udit/anaconda3/envs/master/lib/python3.7/site-packages/scipy/integrate/odepack.py:247: ODEintWarning: Illegal input detected (internal error). Run with full_output = 1 to get quantitative information.\n",
      "  warnings.warn(warning_msg, ODEintWarning)\n",
      "ipykernel_launcher:32: RuntimeWarning: invalid value encountered in double_scalars\n",
      "ipykernel_launcher:33: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/home/udit/anaconda3/envs/master/lib/python3.7/site-packages/scipy/integrate/odepack.py:247: ODEintWarning: Illegal input detected (internal error). Run with full_output = 1 to get quantitative information.\n",
      "  warnings.warn(warning_msg, ODEintWarning)\n",
      "ipykernel_launcher:32: RuntimeWarning: invalid value encountered in double_scalars\n",
      "ipykernel_launcher:33: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/home/udit/anaconda3/envs/master/lib/python3.7/site-packages/scipy/integrate/odepack.py:247: ODEintWarning: Illegal input detected (internal error). Run with full_output = 1 to get quantitative information.\n",
      "  warnings.warn(warning_msg, ODEintWarning)\n",
      "ipykernel_launcher:32: RuntimeWarning: invalid value encountered in double_scalars\n",
      "ipykernel_launcher:33: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/home/udit/anaconda3/envs/master/lib/python3.7/site-packages/scipy/integrate/odepack.py:247: ODEintWarning: Illegal input detected (internal error). Run with full_output = 1 to get quantitative information.\n",
      "  warnings.warn(warning_msg, ODEintWarning)\n",
      "ipykernel_launcher:32: RuntimeWarning: invalid value encountered in double_scalars\n",
      "ipykernel_launcher:33: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/home/udit/anaconda3/envs/master/lib/python3.7/site-packages/scipy/integrate/odepack.py:247: ODEintWarning: Illegal input detected (internal error). Run with full_output = 1 to get quantitative information.\n",
      "  warnings.warn(warning_msg, ODEintWarning)\n",
      "ipykernel_launcher:32: RuntimeWarning: invalid value encountered in double_scalars\n",
      "ipykernel_launcher:33: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/home/udit/anaconda3/envs/master/lib/python3.7/site-packages/scipy/integrate/odepack.py:247: ODEintWarning: Illegal input detected (internal error). Run with full_output = 1 to get quantitative information.\n",
      "  warnings.warn(warning_msg, ODEintWarning)\n",
      "ipykernel_launcher:32: RuntimeWarning: invalid value encountered in double_scalars\n",
      "ipykernel_launcher:33: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/home/udit/anaconda3/envs/master/lib/python3.7/site-packages/scipy/integrate/odepack.py:247: ODEintWarning: Illegal input detected (internal error). Run with full_output = 1 to get quantitative information.\n",
      "  warnings.warn(warning_msg, ODEintWarning)\n",
      "ipykernel_launcher:32: RuntimeWarning: invalid value encountered in double_scalars\n",
      "ipykernel_launcher:33: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/home/udit/anaconda3/envs/master/lib/python3.7/site-packages/scipy/integrate/odepack.py:247: ODEintWarning: Illegal input detected (internal error). Run with full_output = 1 to get quantitative information.\n",
      "  warnings.warn(warning_msg, ODEintWarning)\n",
      "ipykernel_launcher:32: RuntimeWarning: invalid value encountered in double_scalars\n",
      "ipykernel_launcher:33: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/home/udit/anaconda3/envs/master/lib/python3.7/site-packages/scipy/integrate/odepack.py:247: ODEintWarning: Illegal input detected (internal error). Run with full_output = 1 to get quantitative information.\n",
      "  warnings.warn(warning_msg, ODEintWarning)\n",
      "ipykernel_launcher:32: RuntimeWarning: invalid value encountered in double_scalars\n",
      "ipykernel_launcher:33: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/home/udit/anaconda3/envs/master/lib/python3.7/site-packages/scipy/integrate/odepack.py:247: ODEintWarning: Illegal input detected (internal error). Run with full_output = 1 to get quantitative information.\n",
      "  warnings.warn(warning_msg, ODEintWarning)\n",
      "ipykernel_launcher:32: RuntimeWarning: invalid value encountered in double_scalars\n",
      "ipykernel_launcher:33: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/home/udit/anaconda3/envs/master/lib/python3.7/site-packages/scipy/integrate/odepack.py:247: ODEintWarning: Illegal input detected (internal error). Run with full_output = 1 to get quantitative information.\n",
      "  warnings.warn(warning_msg, ODEintWarning)\n",
      "ipykernel_launcher:32: RuntimeWarning: invalid value encountered in double_scalars\n",
      "ipykernel_launcher:33: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/home/udit/anaconda3/envs/master/lib/python3.7/site-packages/scipy/integrate/odepack.py:247: ODEintWarning: Illegal input detected (internal error). Run with full_output = 1 to get quantitative information.\n",
      "  warnings.warn(warning_msg, ODEintWarning)\n",
      "ipykernel_launcher:32: RuntimeWarning: invalid value encountered in double_scalars\n",
      "ipykernel_launcher:33: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/home/udit/anaconda3/envs/master/lib/python3.7/site-packages/scipy/integrate/odepack.py:247: ODEintWarning: Illegal input detected (internal error). Run with full_output = 1 to get quantitative information.\n",
      "  warnings.warn(warning_msg, ODEintWarning)\n",
      "ipykernel_launcher:32: RuntimeWarning: invalid value encountered in double_scalars\n",
      "ipykernel_launcher:33: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/home/udit/anaconda3/envs/master/lib/python3.7/site-packages/scipy/integrate/odepack.py:247: ODEintWarning: Illegal input detected (internal error). Run with full_output = 1 to get quantitative information.\n",
      "  warnings.warn(warning_msg, ODEintWarning)\n",
      "ipykernel_launcher:32: RuntimeWarning: invalid value encountered in double_scalars\n",
      "ipykernel_launcher:33: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/home/udit/anaconda3/envs/master/lib/python3.7/site-packages/scipy/integrate/odepack.py:247: ODEintWarning: Illegal input detected (internal error). Run with full_output = 1 to get quantitative information.\n",
      "  warnings.warn(warning_msg, ODEintWarning)\n",
      "ipykernel_launcher:32: RuntimeWarning: invalid value encountered in double_scalars\n",
      "ipykernel_launcher:33: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/home/udit/anaconda3/envs/master/lib/python3.7/site-packages/scipy/integrate/odepack.py:247: ODEintWarning: Illegal input detected (internal error). Run with full_output = 1 to get quantitative information.\n",
      "  warnings.warn(warning_msg, ODEintWarning)\n",
      "ipykernel_launcher:32: RuntimeWarning: invalid value encountered in double_scalars\n",
      "ipykernel_launcher:33: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/home/udit/anaconda3/envs/master/lib/python3.7/site-packages/scipy/integrate/odepack.py:247: ODEintWarning: Illegal input detected (internal error). Run with full_output = 1 to get quantitative information.\n",
      "  warnings.warn(warning_msg, ODEintWarning)\n",
      "ipykernel_launcher:32: RuntimeWarning: invalid value encountered in double_scalars\n",
      "ipykernel_launcher:33: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/home/udit/anaconda3/envs/master/lib/python3.7/site-packages/scipy/integrate/odepack.py:247: ODEintWarning: Illegal input detected (internal error). Run with full_output = 1 to get quantitative information.\n",
      "  warnings.warn(warning_msg, ODEintWarning)\n",
      "ipykernel_launcher:32: RuntimeWarning: invalid value encountered in double_scalars\n",
      "ipykernel_launcher:33: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/home/udit/anaconda3/envs/master/lib/python3.7/site-packages/scipy/integrate/odepack.py:247: ODEintWarning: Illegal input detected (internal error). Run with full_output = 1 to get quantitative information.\n",
      "  warnings.warn(warning_msg, ODEintWarning)\n",
      "ipykernel_launcher:32: RuntimeWarning: invalid value encountered in double_scalars\n",
      "ipykernel_launcher:33: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/home/udit/anaconda3/envs/master/lib/python3.7/site-packages/scipy/integrate/odepack.py:247: ODEintWarning: Illegal input detected (internal error). Run with full_output = 1 to get quantitative information.\n",
      "  warnings.warn(warning_msg, ODEintWarning)\n",
      "ipykernel_launcher:32: RuntimeWarning: invalid value encountered in double_scalars\n",
      "ipykernel_launcher:33: RuntimeWarning: invalid value encountered in double_scalars\n",
      "/home/udit/anaconda3/envs/master/lib/python3.7/site-packages/scipy/integrate/odepack.py:247: ODEintWarning: Illegal input detected (internal error). Run with full_output = 1 to get quantitative information.\n",
      "  warnings.warn(warning_msg, ODEintWarning)\n"
     ]
    }
   ],
   "source": [
    "Log_Dyna = Write_Log(os.path.join(os.getcwd(), 'Code', '01_Species_Reintroductions', '05_Analysis', 'Dyna_of_SR', 'Log_Dyna.txt'), True)\n",
    "#for CASE_TO_SOLVE, loc in zip(['Real_1D', 'Syn_01', 'Syn_2D', 'Real_2D'],['01','02','03','04']):\n",
    "for CASE_TO_SOLVE, loc in zip(['Real_1D'],['01']):\n",
    "    DATA_TYPE = CASE_TO_SOLVE\n",
    "    CASE_TO_SOLVE_init(DATA_TYPE)\n",
    "    NetworkName_Solved_df = UnpickleObj(os.path.join(os.getcwd(), 'Code', '01_Species_Reintroductions', '01_Simulation_Execution', f\"Network_Solved_{DATA_TYPE.split('_')[0]}.pkl\"))\n",
    "    M = UnpickleObj(os.path.join(os.getcwd(), 'Code', '01_Species_Reintroductions', '01_Simulation_Execution', f\"M_{DATA_TYPE.split('_')[0]}.pkl\"))\n",
    "\n",
    "    if DATA_TYPE.split('_')[0] == 'Syn':\n",
    "        M = {k:M[k] for k in M.keys() if int(k.split('_')[2]) == 100}\n",
    "    '''\n",
    "    with Parallel(n_jobs=5, verbose = 10) as parallel:\n",
    "            data = parallel(delayed(get_Data)(NetworkName,loc) for NetworkName in itertools.islice(M.keys(),0,None,1))\n",
    "    '''\n",
    "    for NetworkName in itertools.islice(M.keys(),1,None,1):\n",
    "        get_Data(NetworkName, loc)\n",
    "        temp = UnpickleObj(os.path.join(os.getcwd(), 'Code', '01_Species_Reintroductions', '05_Analysis', 'Dyna_of_SR', f\"{loc}\", f\"{NetworkName}.pkl\"))\n",
    "        print(NetworkName, ': ', temp.shape)\n",
    "        Log_Dyna.Log_Entry(f\"{NetworkName} : {temp.shape}\")\n",
    "Log_Dyna.close_Log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M_PL_003 :  (14830, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>m</th>\n",
       "      <th>L</th>\n",
       "      <th>S</th>\n",
       "      <th>A</th>\n",
       "      <th>C</th>\n",
       "      <th>N</th>\n",
       "      <th>g_p</th>\n",
       "      <th>g_a</th>\n",
       "      <th>beta_eff_p</th>\n",
       "      <th>beta_eff_a</th>\n",
       "      <th>Abundance</th>\n",
       "      <th>Ob Abundance</th>\n",
       "      <th>Settling Time</th>\n",
       "      <th>Persistence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>26</td>\n",
       "      <td>23</td>\n",
       "      <td>61.0</td>\n",
       "      <td>49</td>\n",
       "      <td>0.884615</td>\n",
       "      <td>0.102007</td>\n",
       "      <td>0.169146</td>\n",
       "      <td>1.440472</td>\n",
       "      <td>1.509306</td>\n",
       "      <td>0.588246</td>\n",
       "      <td>0.605079</td>\n",
       "      <td>0.131042</td>\n",
       "      <td>0.131042</td>\n",
       "      <td>52</td>\n",
       "      <td>0.722222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27</td>\n",
       "      <td>23</td>\n",
       "      <td>62.0</td>\n",
       "      <td>50</td>\n",
       "      <td>0.851852</td>\n",
       "      <td>0.099839</td>\n",
       "      <td>0.168488</td>\n",
       "      <td>1.424158</td>\n",
       "      <td>1.516766</td>\n",
       "      <td>0.573875</td>\n",
       "      <td>0.605079</td>\n",
       "      <td>0.130789</td>\n",
       "      <td>0.130789</td>\n",
       "      <td>52</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>23</td>\n",
       "      <td>63.0</td>\n",
       "      <td>51</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.097826</td>\n",
       "      <td>0.167618</td>\n",
       "      <td>1.409009</td>\n",
       "      <td>1.523821</td>\n",
       "      <td>0.560083</td>\n",
       "      <td>0.605079</td>\n",
       "      <td>0.130554</td>\n",
       "      <td>0.130554</td>\n",
       "      <td>51</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29</td>\n",
       "      <td>23</td>\n",
       "      <td>64.0</td>\n",
       "      <td>52</td>\n",
       "      <td>0.793103</td>\n",
       "      <td>0.095952</td>\n",
       "      <td>0.166566</td>\n",
       "      <td>1.394906</td>\n",
       "      <td>1.530532</td>\n",
       "      <td>0.546866</td>\n",
       "      <td>0.605079</td>\n",
       "      <td>0.130334</td>\n",
       "      <td>0.130334</td>\n",
       "      <td>51</td>\n",
       "      <td>0.805556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30</td>\n",
       "      <td>23</td>\n",
       "      <td>65.0</td>\n",
       "      <td>53</td>\n",
       "      <td>0.766667</td>\n",
       "      <td>0.094203</td>\n",
       "      <td>0.165359</td>\n",
       "      <td>1.381742</td>\n",
       "      <td>1.536944</td>\n",
       "      <td>0.534206</td>\n",
       "      <td>0.605079</td>\n",
       "      <td>0.130129</td>\n",
       "      <td>0.130129</td>\n",
       "      <td>51</td>\n",
       "      <td>0.833333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    n   m     L   S         A         C         N       g_p       g_a  \\\n",
       "0  26  23  61.0  49  0.884615  0.102007  0.169146  1.440472  1.509306   \n",
       "1  27  23  62.0  50  0.851852  0.099839  0.168488  1.424158  1.516766   \n",
       "2  28  23  63.0  51  0.821429  0.097826  0.167618  1.409009  1.523821   \n",
       "3  29  23  64.0  52  0.793103  0.095952  0.166566  1.394906  1.530532   \n",
       "4  30  23  65.0  53  0.766667  0.094203  0.165359  1.381742  1.536944   \n",
       "\n",
       "   beta_eff_p  beta_eff_a  Abundance  Ob Abundance  Settling Time  Persistence  \n",
       "0    0.588246    0.605079   0.131042      0.131042             52     0.722222  \n",
       "1    0.573875    0.605079   0.130789      0.130789             52     0.750000  \n",
       "2    0.560083    0.605079   0.130554      0.130554             51     0.777778  \n",
       "3    0.546866    0.605079   0.130334      0.130334             51     0.805556  \n",
       "4    0.534206    0.605079   0.130129      0.130129             51     0.833333  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp = UnpickleObj(os.path.join(os.getcwd(), 'Code', '01_Species_Reintroductions', '05_Analysis', 'Dyna_of_SR', f\"{loc}\", f\"{NetworkName}.pkl\"))\n",
    "print(NetworkName, ': ', temp.shape)\n",
    "temp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for NetworkName in itertools.islice(M.keys(),0,None,1):\n",
    "    temp = UnpickleObj(os.path.join(os.getcwd(), 'Code', '01_Species_Reintroductions', '05_Analysis', 'Dyna_of_SR', f\"{loc}\", f\"{NetworkName}.pkl\"))\n",
    "    print(NetworkName, ': ', temp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = UnpickleObj(os.path.join(os.getcwd(), 'Code', '01_Species_Reintroductions', '05_Analysis', 'Dyna_of_SR', \"01\", \"M_PL_003.pkl\"))\n",
    "print(X.shape)\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UPDATE_SAVED = True\n",
    "for CASE_TO_SOLVE, loc in zip(['Real_1D', 'Syn_1D', 'Syn_2D', 'Real_2D'],['01','02','03','04']):\n",
    "    DATA_TYPE = CASE_TO_SOLVE\n",
    "    CASE_TO_SOLVE_init(DATA_TYPE)\n",
    "    NetworkName_Solved_df = UnpickleObj(os.path.join(os.getcwd(), 'Code', '01_Species_Reintroductions', '01_Simulation_Execution', f\"Network_Solved_{DATA_TYPE.split('_')[0]}.pkl\"))\n",
    "    M = UnpickleObj(os.path.join(os.getcwd(), 'Code', '01_Species_Reintroductions', '01_Simulation_Execution', f\"M_{DATA_TYPE.split('_')[0]}.pkl\"))\n",
    "    if DATA_TYPE.split('_')[0] == 'Syn':\n",
    "        M = {k:M[k] for k in M.keys() if int(k.split('_')[2]) == 100}\n",
    "    D = {}\n",
    "    DF = pd.DataFrame([],columns = ['n','m','L','S','A','C','N','g_p','g_a','beta_eff_p','beta_eff_a','Abundance','Ob Abundance','Settling Time','Persistence'])\n",
    "\n",
    "    for NetworkName in itertools.islice(M.keys(),0,None,1):\n",
    "        X = UnpickleObj(os.path.join(os.getcwd(), 'Code', '01_Species_Reintroductions', '05_Analysis', \"Dyna_of_SR\", f\"{loc}\", f\"{NetworkName}.pkl\"))\n",
    "        #X = UnpickleObj(f\"Dyna_of_SR/{loc}/{NetworkName}.pkl\")\n",
    "        X = X.apply(pd.to_numeric)\n",
    "        X.drop(X[ X['Ob Abundance'] == 0 ].index ,inplace = True)\n",
    "        Y1 = X.corr(method = 'pearson')\n",
    "        Y2 = X.corr(method = 'kendall')\n",
    "        Y3 = X.corr(method = 'spearman')\n",
    "        D[NetworkName] = [Y1,Y2,Y3]\n",
    "        DF = pd.concat([DF,X], ignore_index = True)\n",
    "        print(DF.shape)\n",
    "    del NetworkName, X, Y1, Y2, Y3, M, NetworkName_Solved_df\n",
    "\n",
    "    DF = DF.apply(pd.to_numeric)\n",
    "    DF.sort_values(by = ['Abundance', 'Ob Abundance'], axis = 0, inplace = True, ignore_index = True)\n",
    "    DF.drop(DF[ DF['Ob Abundance'] == 0 ].index ,inplace = True)\n",
    "    DF.reset_index(drop = True, inplace = True)\n",
    "    print(DATA_TYPE, DF.shape)\n",
    "    Z = DF.describe()\n",
    "    Y1 = DF.corr(method = 'pearson')\n",
    "    Y2 = DF.corr(method = 'kendall')\n",
    "    Y3 = DF.corr(method = 'spearman')\n",
    "    Data = [Z, Y1, Y2, Y3, DF, D]\n",
    "    if UPDATE_SAVED:\n",
    "        PickleObj(Data, os.path.join(os.getcwd(), 'Code', '01_Species_Reintroductions', '05_Analysis', 'Dyna_of_SR', f\"Data_{loc}_Consolidated.pkl\"))\n",
    "del loc, CASE_TO_SOLVE, DF, Z, Y1, Y2, Y3, Data\n",
    "del D, UPDATE_SAVED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for CASE_TO_SOLVE, loc in zip(['Real_1D'],['01']):\n",
    "    DATA_TYPE = CASE_TO_SOLVE\n",
    "    CASE_TO_SOLVE_init(DATA_TYPE)\n",
    "    NetworkName_Solved_df = UnpickleObj(os.path.join(os.getcwd(), 'Code', '01_Species_Reintroductions', '01_Simulation_Execution', f\"Network_Solved_{DATA_TYPE.split('_')[0]}.pkl\"))\n",
    "    M = UnpickleObj(os.path.join(os.getcwd(), 'Code', '01_Species_Reintroductions', '01_Simulation_Execution', f\"M_{DATA_TYPE.split('_')[0]}.pkl\"))\n",
    "    if DATA_TYPE.split('_')[0] == 'Syn':\n",
    "        M = {k:M[k] for k in M.keys() if int(k.split('_')[2]) == 100}\n",
    "    D = {}\n",
    "    DF = pd.DataFrame([],columns = ['n','m','L','S','A','C','N','g_p','g_a','beta_eff_p','beta_eff_a','Abundance','Ob Abundance','Settling Time','Persistence'])\n",
    "\n",
    "    for NetworkName in itertools.islice(M.keys(),0,1,1):\n",
    "        X = UnpickleObj(os.path.join(os.getcwd(), 'Code', '01_Species_Reintroductions', '05_Analysis', \"Dyna_of_SR\", f\"{loc}\", f\"{NetworkName}.pkl\"))\n",
    "        print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FileName = os.path.join(os.getcwd(), 'Code', '01_Species_Reintroductions', '05_Analysis', 'Dyna_of_SR', \"Data_01_Consolidated.pkl\")\n",
    "D01 = UnpickleObj(FileName)[4]\n",
    "D01.drop(D01[ D01['N'] == 0 ].index ,inplace = True)\n",
    "D01.dropna(axis = 0, inplace = True)\n",
    "D01 = D01[D01['Abundance'] > 1.0]\n",
    "D01.reset_index(drop = True, inplace = True)\n",
    "\n",
    "FileName = os.path.join(os.getcwd(), 'Code', '01_Species_Reintroductions', '05_Analysis', 'Dyna_of_SR', \"Data_02_Consolidated.pkl\")\n",
    "D02 = UnpickleObj(FileName)[4]\n",
    "D02.drop(D02[ D02['N'] == 0 ].index ,inplace = True)\n",
    "D02.dropna(axis = 0, inplace = True)\n",
    "D02 = D02[D02['Abundance'] > 1.0]\n",
    "D02.reset_index(drop = True, inplace = True)\n",
    "\n",
    "FileName = os.path.join(os.getcwd(), 'Code', '01_Species_Reintroductions', '05_Analysis', 'Dyna_of_SR', \"Data_03_Consolidated.pkl\")\n",
    "D03 = UnpickleObj(FileName)[4]\n",
    "D03.drop(D03[ D03['N'] == 0 ].index ,inplace = True)\n",
    "D03.dropna(axis = 0, inplace = True)\n",
    "D03 = D03[D03['Abundance'] > 0.3]\n",
    "D03.reset_index(drop = True, inplace = True)\n",
    "\n",
    "FileName = os.path.join(os.getcwd(), 'Code', '01_Species_Reintroductions', '05_Analysis', 'Dyna_of_SR', \"Data_04_Consolidated.pkl\")\n",
    "D04 = UnpickleObj(FileName)[4]\n",
    "D04.drop(D04[ D04['N'] == 0 ].index ,inplace = True)\n",
    "D04.dropna(axis = 0, inplace = True)\n",
    "D04 = D04[D04['Abundance'] > 0.3]\n",
    "D04.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(D01.shape)\n",
    "print(D02.shape)\n",
    "print(D03.shape)\n",
    "print(D04.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debugging the Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for CASE_TO_SOLVE, loc in zip(['Real_1D'],['01']):\n",
    "    DATA_TYPE = CASE_TO_SOLVE\n",
    "    CASE_TO_SOLVE_init(DATA_TYPE)\n",
    "    NetworkName_Solved_df = UnpickleObj(os.path.join(os.getcwd(), 'Code', '01_Species_Reintroductions', '01_Simulation_Execution', f\"Network_Solved_{DATA_TYPE.split('_')[0]}.pkl\"))\n",
    "    M = UnpickleObj(os.path.join(os.getcwd(), 'Code', '01_Species_Reintroductions', '01_Simulation_Execution', f\"M_{DATA_TYPE.split('_')[0]}.pkl\"))\n",
    "    if DATA_TYPE.split('_')[0] == 'Syn':\n",
    "        M = {k:M[k] for k in M.keys() if int(k.split('_')[2]) == 100}\n",
    "Data_Scatter = []\n",
    "#print(NetworkName)\n",
    "NetworkName = 'M_PL_041'\n",
    "M_df = M[NetworkName]\n",
    "#data_two = UnpickleObj(os.path.join(os.getcwd(), 'Code', '01_Species_Reintroductions', '02_Generate_Database', f\"{DATA_TYPE}_Data\", f\"{NetworkName}\", f\"{NetworkName}-Data_two.pkl\"))\n",
    "#data_two = UnpickleObj(os.path.join(r\"D:\\WORK\\EN\\Documents\\Ecological_Restoration_01\\Real_Data\", f\"{NetworkName}\\{NetworkName}-Data_two.pkl\"))\n",
    "data_two = UnpickleObj(f\"/home/udit/Documents/Ecological_Restoration_{loc}/{DATA_TYPE.split('_')[0]}_Data/{NetworkName}/{NetworkName}-Data_two.pkl\")\n",
    "Keys_Filter = [[0,1,2], [2,5,8], [0,2]]\n",
    "Ensembles_batch = [0,10]\n",
    "data_two = {k: v[Ensembles_batch[0]:min(Ensembles_batch[1],len(v))] for k,v in data_two.items() if k in set(list(itertools.product(*Keys_Filter)))}\n",
    "del Keys_Filter, Ensembles_batch\n",
    "P = {i:j for i,j in zip(['a', 'b', 'mu', 'h', 'g0', 't', 'k'],[0.3, 1.0, 1e-4, 0.7, 1.0, 0.5, 0.0])}\n",
    "\n",
    "def G_iter(data_two):\n",
    "    for key in list(data_two.keys()):\n",
    "        for ensembles_ID in range(len(data_two[key])):\n",
    "            yield (key, ensembles_ID, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for approach in ['four']:\n",
    "    for key, E, T in itertools.islice(G_iter(data_two),0,1,1):\n",
    "        print(approach, key, E, T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = UnpickleObj(f\"/home/udit/Documents/Ecological_Restoration_{loc}/{DATA_TYPE.split('_')[0]}_Data/{NetworkName}/{NetworkName}-Data_{approach}_{key}_{E}_{T}.pkl\")\n",
    "print(type(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_per = RemovalFromNodeList(data_two[key][E],M_df,axis=key[2])\n",
    "M_per = M_per.loc[(M_per!=0).any(axis=1)] #Cleaning Row isoloates\n",
    "M_per = M_per.loc[:, (M_per!=0).any(axis=0)] #Cleaning Column isoloates\n",
    "print(M_per.shape)\n",
    "M_per"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "M_per_dict = {}\n",
    "            \n",
    "Degree_P = list(np.sum(np.array(M_per), axis = 1))\n",
    "Degree_A = list(np.sum(np.array(M_per), axis = 0))\n",
    "g_p = np.sum(P['g0'] * np.power(np.array(Degree_P),1-P['t'])) / len(Degree_P)\n",
    "g_a = np.sum(P['g0'] * np.power(np.array(Degree_A),1-P['t'])) / len(Degree_A)\n",
    "M_per_dict['g_p'] = g_p\n",
    "M_per_dict['g_a'] = g_a\n",
    "beta_eff_p, _, _ = ODE_Solve_Low(t0=0,tf=100,num=50,x_low=0,A=np.array(getProjectedNet(M_per, flag = [True, False])[0]))\n",
    "beta_eff_a, _, _ = ODE_Solve_Low(t0=0,tf=100,num=50,x_low=0,A=np.array(getProjectedNet(M_per, flag = [False, True])[1]))\n",
    "M_per_dict['beta_eff_p'] = beta_eff_p\n",
    "M_per_dict['beta_eff_a'] = beta_eff_a\n",
    "\n",
    "M_per_dict['n'] = M_per.shape[0]\n",
    "M_per_dict['m'] = M_per.shape[1]\n",
    "M_per_dict['L'] = sum(np.array(M_per).flatten())\n",
    "M_per_dict['S'] = M_per_dict['n'] + M_per_dict['m']\n",
    "M_per_dict['A'] = M_per_dict['m'] / M_per_dict['n']\n",
    "M_per_dict['C'] = M_per_dict['L'] / (M_per_dict['m']*M_per_dict['n'])\n",
    "M_per_dict['N'] = NestednessCalculator(np.array(M_per)).nodf(np.array(M_per))\n",
    "\n",
    "Z1 = np.mean(np.array(X[3][0][-1,:]))\n",
    "Z2 = np.mean(np.array(list(map(isExtinct,X[3][0][-1,:]))))\n",
    "Z3 = getResilience(X[3][0])\n",
    "Z4 = getPersistence(np.array(list(map(isExtinct,X[3][0][-1,:])))) / M_df.shape[T]\n",
    "\n",
    "temp = [M_per_dict[x] for x in ['n','m','L','S','A','C','N','g_p','g_a','beta_eff_p','beta_eff_a']] + [Z1,Z2,Z3,Z4]\n",
    "Data_Scatter += [temp]\n",
    "print(Data_Scatter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for Step in list(range(1,len(X[0])+1)):\n",
    "    M_per = M_per.append(M_df.loc[X[0][Step-1]])\n",
    "    M_per[M_per.isnull()] = M_df\n",
    "    M_per = M_per.loc[(M_per!=0).any(axis=1)] #Cleaning Row isoloates\n",
    "    M_per = M_per.loc[:, (M_per!=0).any(axis=0)] #Cleaning Column isoloates\n",
    "    \n",
    "    Degree_P = list(np.sum(np.array(M_per), axis = 1))\n",
    "    Degree_A = list(np.sum(np.array(M_per), axis = 0))\n",
    "    g_p = np.sum(P['g0'] * np.power(np.array(Degree_P),1-P['t'])) / len(Degree_P)\n",
    "    g_a = np.sum(P['g0'] * np.power(np.array(Degree_A),1-P['t'])) / len(Degree_A)\n",
    "    M_per_dict['g_p'] = g_p\n",
    "    M_per_dict['g_a'] = g_a\n",
    "    beta_eff_p, _, _ = ODE_Solve_Low(t0=0,tf=100,num=50,x_low=0,A=np.array(getProjectedNet(M_per, flag = [True, False])[0]))\n",
    "    beta_eff_a, _, _ = ODE_Solve_Low(t0=0,tf=100,num=50,x_low=0,A=np.array(getProjectedNet(M_per, flag = [False, True])[1]))\n",
    "    M_per_dict['beta_eff_p'] = beta_eff_p\n",
    "    M_per_dict['beta_eff_a'] = beta_eff_a\n",
    "\n",
    "    M_per_dict['n'] = M_per.shape[0]\n",
    "    M_per_dict['m'] = M_per.shape[1]\n",
    "    M_per_dict['L'] = sum(np.array(M_per).flatten())\n",
    "    M_per_dict['S'] = M_per_dict['n'] + M_per_dict['m']\n",
    "    M_per_dict['A'] = M_per_dict['m'] / M_per_dict['n']\n",
    "    M_per_dict['C'] = M_per_dict['L'] / (M_per_dict['m']*M_per_dict['n'])\n",
    "    M_per_dict['N'] = NestednessCalculator(np.array(M_per)).nodf(np.array(M_per))\n",
    "    \n",
    "    Z1 = np.mean(np.array(X[3][Step][-1,:]))\n",
    "    Z2 = np.mean(np.array(list(map(isExtinct,X[3][Step][-1,:]))))\n",
    "    Z3 = getResilience(X[3][Step])\n",
    "    Z4 = getPersistence(np.array(list(map(isExtinct,X[3][Step][-1,:])))) / M_df.shape[T]\n",
    "    \n",
    "    temp = [M_per_dict[x] for x in ['n','m','L','S','A','C','N','g_p','g_a','beta_eff_p','beta_eff_a']] + [Z1,Z2,Z3,Z4]\n",
    "    Data_Scatter += [temp]\n",
    "    counter += 1\n",
    "    print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = np.array(Data_Scatter)\n",
    "print(temp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF = pd.DataFrame(Data_Scatter, columns = ['n','m','L','S','A','C','N','g_p','g_a','beta_eff_p','beta_eff_a','Abundance','Ob Abundance','Settling Time','Persistence'])\n",
    "print(DF.shape)\n",
    "DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF.drop_duplicates(inplace=True, ignore_index=True)\n",
    "print(DF.shape)\n",
    "DF.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FileName = os.path.join(os.getcwd(), 'Code', '01_Species_Reintroductions', '05_Analysis', 'Dyna_of_SR', f\"{loc}\", f\"{NetworkName}.pkl\")\n",
    "os.makedirs(os.path.dirname(FileName), exist_ok=True)\n",
    "PickleObj(DF,FileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DF = UnpickleObj(FileName)\n",
    "print(DF.shape)\n",
    "DF.head()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f5dcb542c55fb5e0f34812ee120ad81aaa91d2baace64f600af55ed1e6701a2a"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('master': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
